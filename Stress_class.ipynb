{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f45016-5001-4919-a047-3d98de0b62d5",
   "metadata": {},
   "source": [
    "### Data preparation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "207b095c-bf9a-4f5b-b09b-6779ed5f2581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1500 frames from C:\\Users\\Hassan\\Desktop\\vid.mp4 to output_frames\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Open the video file\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    success, image = vidcap.read()\n",
    "    count = 0\n",
    "\n",
    "    # Loop through the video frames\n",
    "    while success:\n",
    "        # Save the frame as a JPEG file\n",
    "        frame_path = os.path.join(output_folder, f\"frame{count}.jpg\")\n",
    "        cv2.imwrite(frame_path, image)\n",
    "        \n",
    "        # Read the next frame\n",
    "        success, image = vidcap.read()\n",
    "        count += 1\n",
    "\n",
    "    print(f\"Extracted {count} frames from {video_path} to {output_folder}\")\n",
    "\n",
    "# Example usage\n",
    "video_path = r'C:\\Users\\Hassan\\Desktop\\vid.mp4'\n",
    "output_folder = 'output_frames'\n",
    "extract_frames(video_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83947623-9ba0-4a1b-8e6f-b8e92af0b5cb",
   "metadata": {},
   "source": [
    "#### Face extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b69115f-1f06-4249-a019-b51247e58ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_faces(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    for filename in os.listdir(input_folder):\n",
    "        img_path = os.path.join(input_folder, filename)\n",
    "        image = cv2.imread(img_path)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            face = image[y:y+h, x:x+w]\n",
    "            face_path = os.path.join(output_folder, filename)\n",
    "            cv2.imwrite(face_path, face)\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'output_frames'\n",
    "output_folder = 'faces'\n",
    "extract_faces(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e90b9-d902-4cc3-a88a-21a1e178debd",
   "metadata": {},
   "source": [
    "#### Resizing frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3229b7d9-b783-413b-83a5-b7daf1695c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def resize_images(input_folder, output_folder, size=(224, 224)):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for filename in os.listdir(input_folder):\n",
    "        img_path = os.path.join(input_folder, filename)\n",
    "        image = Image.open(img_path)\n",
    "        image = image.resize(size)\n",
    "        image.save(os.path.join(output_folder, filename))\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'faces'\n",
    "output_folder = 'resized_faces'\n",
    "resize_images(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8509bf05-931f-469a-9b96-9fd787818f7f",
   "metadata": {},
   "source": [
    "#### Normalize the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2755516-5672-44cf-b45e-a4fdffdb61da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_images(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for filename in os.listdir(input_folder):\n",
    "        img_path = os.path.join(input_folder, filename)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = image / 255.0  # Normalize pixel values\n",
    "        normalized_path = os.path.join(output_folder, filename)\n",
    "        cv2.imwrite(normalized_path, (image * 255).astype(np.uint8))\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'resized_faces'\n",
    "output_folder = 'normalized_faces'\n",
    "normalize_images(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba7efb-ed16-4c34-9a7f-f5058c2ba671",
   "metadata": {},
   "source": [
    "### Labelling the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db9ccdc8-d7b4-4714-8d1e-574214d46a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "input_folder = r'C:\\Users\\Hassan\\Desktop\\ML_DL\\myenv\\normalized_faces'  # Folder with normalized frames\n",
    "output_csv = r'C:\\Users\\Hassan\\Desktop\\ML_DL\\myenv\\labels.csv'  # CSV file to save labels\n",
    "\n",
    "# Create/Open the CSV file for writing\n",
    "with open(output_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['frame_path', 'label'])  # Write header\n",
    "\n",
    "    # Iterate through all images in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            img_path = os.path.join(input_folder, filename)\n",
    "            frame_number = int(filename.split('frame')[1].split('.')[0])  # Extract frame number\n",
    "            \n",
    "            # Label based on the provided ranges\n",
    "            if 0 <= frame_number <= 269:\n",
    "                label = 0  # Low Stress\n",
    "            elif 270 <= frame_number <= 313:\n",
    "                label = 1  # Medium Stress\n",
    "            elif 314 <= frame_number <= 656:\n",
    "                label = 0  # Low Stress\n",
    "            elif 657 <= frame_number <= 676:\n",
    "                label = 1  # Medium Stress\n",
    "            elif 677 <= frame_number <= 730:\n",
    "                label = 2  # High Stress\n",
    "            elif 731 <= frame_number <= 750:\n",
    "                label = 1  # Medium Stress\n",
    "            elif 809 <= frame_number <= 956:\n",
    "                label = 2  # High Stress\n",
    "            elif 991 <= frame_number <= 1003:\n",
    "                label = 2  # High Stress\n",
    "            else:\n",
    "                label = ''  # Validation set (leave blank)\n",
    "\n",
    "            # Write the image path and label to CSV\n",
    "            writer.writerow([img_path, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b74d4b7-b7d6-4649-90fb-a42ccd36f570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted CSV file saved to sorted_labels.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV file\n",
    "csv_file = 'labels.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Sort the DataFrame\n",
    "df_sorted = df.sort_values(by=['label'], na_position='last')\n",
    "\n",
    "# Save the sorted DataFrame to a new CSV file\n",
    "sorted_csv_file = 'sorted_labels.csv'\n",
    "df_sorted.to_csv(sorted_csv_file, index=False)\n",
    "\n",
    "print(f\"Sorted CSV file saved to {sorted_csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd5c18-6478-449d-bf33-e84651a01f6f",
   "metadata": {},
   "source": [
    "#### Splitting dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63bd3464-de5b-4d8f-9339-9d68cc1e4449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_images: (425, 224, 224, 3)\n",
      "Shape of val_images: (107, 224, 224, 3)\n",
      "Validation labels (before): (107,)\n",
      "Validation labels (after): (107,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(r'C:\\Users\\Hassan\\Desktop\\ML_DL\\myenv\\labels.csv')\n",
    "\n",
    "# Filter out rows where label is not null\n",
    "data = data.dropna(subset=['label'])\n",
    "\n",
    "# Initialize lists to hold images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Iterate through the data and load images and labels\n",
    "for index, row in data.iterrows():\n",
    "    img_path = str(row['frame_path'])  # Ensure path is converted to string\n",
    "    label = row['label']\n",
    "    \n",
    "    if os.path.exists(img_path):  # Check if the path exists\n",
    "        # Load and preprocess the image\n",
    "        image = load_img(img_path, target_size=(224, 224))\n",
    "        image = img_to_array(image)\n",
    "        image = image.astype('float32') / 255.0\n",
    "\n",
    "        images.append(image)\n",
    "        labels.append(int(label))\n",
    "    else:\n",
    "        print(f\"File not found: {img_path}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure we have non-empty val_labels\n",
    "print(f\"Shape of train_images: {train_images.shape}\")\n",
    "print(f\"Shape of val_images: {val_images.shape}\")\n",
    "print(f\"Validation labels (before): {val_labels.shape}\")\n",
    "\n",
    "# Correct the validation labels assignment\n",
    "val_labels = val_labels[:len(val_images)]\n",
    "\n",
    "print(f\"Validation labels (after): {val_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658aba1-54ba-4a3c-973c-b9ed0ee3a3c2",
   "metadata": {},
   "source": [
    "### Training - PHASE 1 (CNN's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bb0c0cf-666c-43e1-9840-c141b10ca646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15bb18cb-10a6-4346-9176-9577f9fb7481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hassan\\Desktop\\ML_DL\\myenv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18432</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">9,437,696</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,539</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m147,584\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18432\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m9,437,696\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │           \u001b[38;5;34m1,539\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,680,067</span> (36.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,680,067\u001b[0m (36.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,680,067</span> (36.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,680,067\u001b[0m (36.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # Assuming 3 output classes (0, 1, 2)\n",
    "])\n",
    "\n",
    "# Compile the mode\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5afe3c91-d9a7-497e-b589-ebf2ed265473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.4825 - loss: 1.1252 - val_accuracy: 0.6355 - val_loss: 0.6935\n",
      "Epoch 2/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.7397 - loss: 0.5354 - val_accuracy: 0.9065 - val_loss: 0.1972\n",
      "Epoch 3/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.8836 - loss: 0.3190 - val_accuracy: 0.8785 - val_loss: 0.2679\n",
      "Epoch 4/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.9119 - loss: 0.2075 - val_accuracy: 0.9626 - val_loss: 0.1153\n",
      "Epoch 5/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.9194 - loss: 0.2230 - val_accuracy: 0.9159 - val_loss: 0.2046\n",
      "Epoch 6/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.9277 - loss: 0.1740 - val_accuracy: 0.9252 - val_loss: 0.1622\n",
      "Epoch 7/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.9312 - loss: 0.1832 - val_accuracy: 0.9439 - val_loss: 0.1012\n",
      "Epoch 8/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9587 - loss: 0.1174 - val_accuracy: 0.9439 - val_loss: 0.1285\n",
      "Epoch 9/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.9568 - loss: 0.0991 - val_accuracy: 0.9439 - val_loss: 0.0856\n",
      "Epoch 10/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - accuracy: 0.9746 - loss: 0.0629 - val_accuracy: 0.9252 - val_loss: 0.1656\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235ms/step - accuracy: 0.9180 - loss: 0.1747\n",
      "Validation Accuracy: 92.52%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_data=(val_images, val_labels))\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(val_images, val_labels)\n",
    "\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f165d-c2c1-4cd0-a829-f88ff5f95fd7",
   "metadata": {},
   "source": [
    "### Optimizing Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9682cd6-e2c0-464a-99ea-872fc198fb20",
   "metadata": {},
   "source": [
    "#### Using face ROI Detection method for face detection (instead of openCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e3c0d-dc2b-4fde-91b4-cded08685041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dlib\n",
    "# import cv2\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Initialize Dlib's face detector\n",
    "# detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# # Function to detect faces and return the ROIs\n",
    "# def detect_faces_dlib(image):\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "#     faces = detector(gray)  # Detect faces\n",
    "#     rois = [image[d.top():d.bottom(), d.left():d.right()] for d in faces]  # Extract face ROIs\n",
    "#     return rois\n",
    "\n",
    "# # Load your trained model\n",
    "# model = load_model('Desktop/ML_DL/my_model.keras')\n",
    "\n",
    "# # Function to preprocess the face for the model\n",
    "# def preprocess_face(face):\n",
    "#     face = cv2.resize(face, (224, 224))  # Resize to the model's input size\n",
    "#     face = face.astype('float32') / 255.0  # Normalize pixel values\n",
    "#     face = np.expand_dims(face, axis=0)  # Add batch dimension\n",
    "#     return face\n",
    "\n",
    "# # Path to the folder containing frames\n",
    "# frames_folder = 'output_frames'\n",
    "\n",
    "# # Iterate through each frame\n",
    "# for filename in os.listdir(frames_folder):\n",
    "#     img_path = os.path.join(frames_folder, filename)\n",
    "#     image = cv2.imread(img_path)\n",
    "#     rois = detect_faces_dlib(image)\n",
    "    \n",
    "#     for i, roi in enumerate(rois):\n",
    "#         preprocessed_face = preprocess_face(roi)\n",
    "#         predictions = model.predict(preprocessed_face)\n",
    "#         predicted_label = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "#         # Map predicted label to stress level\n",
    "#         stress_levels = {0: 'Low Stress', 1: 'Medium Stress', 2: 'High Stress'}\n",
    "#         predicted_stress_level = stress_levels[predicted_label]\n",
    "        \n",
    "#         # Print or save the result\n",
    "#         print(f\"Frame {filename} - Face {i+1}: {predicted_stress_level}\")\n",
    "#         cv2.imshow(f'Face {i+1} - {predicted_stress_level}', roi)\n",
    "#         cv2.waitKey(1000)  # Display for 1 second\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa34531-8ba9-4df2-a019-463e4794fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install dlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
